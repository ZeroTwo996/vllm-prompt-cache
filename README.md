# vllm-prompt-cache

基于开源GPTCache，实现vLLM推理框架的请求Prompt结果缓存，以提升推理服务系统的响应速度